# Ollama


Ollama позволяет запускать текстовые нейромодели локально. 

## Установка в Docker

```
docker pull ollama/ollama

# Запуск на CPU
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

# Запуск на GPU
docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

# Запустить модель
docker exec -it ollama ollama run модель

# Или так, имея доступ к оболочке
docker exec -it ollama bin/bash
ollama run модель
```

Загрузить модели можно [отсюда](https://ollama.com/library).

При загрузке моделей нужно обращать внимание на параметры. Обычно указывается число и буква B. Например 3B, 1.5B.

Для работы моделей 7B вам потребуется не менее 8 ГБ оперативной памяти, для работы моделей 13B — 16 ГБ, а для работы моделей 33B — 32 ГБ.


## Кастомизация ответов

Есть возможность кастомизировать ответы нейросети. Например, сделать так, чтобы она отвечала как персонаж из мультика или герой фильма или игры. Для этого нужно создать файл без расширения Modelfile.

```
docker exec -it ollama bin/bash
touch Modelfile
```

Далее нужно открыть Modelfile и вписать туда примерно следующее - это простой вариант использования.

```
FROM названиеМодели
PARAMETER temperature 1
SYSTEM "Запрос. Например - ты выполняешь роль преподавателя."
```

Дальше нужно создать и запустить созданную модель.

```
ollama create имяДляНовойМодели -f Modelfile
ollama run имяДляНовойМодели
```



## Modelfile

В этом разделе будет подробная информация о содержимом Modelfile. Общая схема инструкций выглядит следующим образом:

```
# комментарий
ИНСТРУКЦИЯ аргументы
```


| Инструкция         | Описание и применение                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| FROM               | Базовая модель. Сюда передается модель, на основе которой будет генерация<br><br>Базовый пример<br>FROM имяМодели:тег<br><br>Для загрузки safetensors моделей<br>FROM путьКМодели                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| PARAMETER <br><br> | Параметры, с которыми будет запускаться модель<br><br>Пример<br>PARAMETER параметр значение<br><br>Некоторые параметры<br>temperature - чем выше значение тем креативнее ответ. Есть риск фантазирования при повышении значения. По умолчанию 0.8<br>seed - сид. Если явно установлен, при повторной генерации будет точно такой же ответ<br>top_k - вероятность бессмыслицы. По умолчанию 40<br>top_p - разнообразие в тексте. Более низкие значения делают ответ более сфокусированным. По умолчанию 0.9<br>mirostat - "Регулятор творчества". Чем больше, тем непредсказуемые ответы. Максимально 2.<br>mirostat_eta - Регулирует, насколько быстро нейросеть учится на том, о чем говорит в данный момент. Более низкие значения сделают ответ более осторожным. Пример mirostat_eta 0.1.<br>mirostat_tau - при низких значениях нейросеть придерживается темы разговора.<br>num_ctx - Длина контекста. По умолчанию 4096.<br>repeat_penalty - не позволяет нейросети повторяться. Например 1.1.<br>stop - Слово, при котором нейросеть перестает говорить. Например parameter stop "Нейросеть".<br>tfs_z - Стремится сделать мысли нейросети менее случайными.<br>num_predict - Ограничивает объем ответа нейросети делая его более кратким. |
| MESSAGE            | Позволяет передать историю сообщений пользователя и собеседника, чтобы нейросеть обучилась как отвечать пользователю. Свойство system является альтернативой SYSTEM. user - сообщение пользователя.  assistant - сообщение нейросети. <br><br>Пример запроса<br><br>MESSAGE user Сообщение<br>MESSAGE assistant Сщщбщение<br>...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
|                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
|                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |


## Использование модуля ollama

Для Python есть модуль [ollama](https://github.com/ollama/ollama-python) для более простого взаимодействия.

Установка
```
pip install ollama
```

Для правильной работы убедитесь, что ollama запущена, а нужная модель загружена.

Следующий код выведет ответ ИИ
```python
from ollama import chat, ChatResponse
response: ChatResponse = chat(model='модель', messages=[{'role': 'user','content': 'Промпт'}])
print(response.message.content)
```

Так же можно выводить в режиме Стриминга - текст будет выводиться как в ИИ типа ChatGPT.
```python
from ollama import chat
stream = chat(model='модель',messages=[{'role': 'user', 'content': 'промпт'}],stream=True,)
for chunk in stream:
    print(chunk['message']['content'], end='', flush=True)
```

### Контекст

Приведенный ниже алгоритм позволит создать простого чат-бота с контекстом. Тут логика - в смене ролей - системный промпт, пользователь и ассистент.
```python
from ollama import chat, ChatResponse
messages = [{"role": "system","content": "Системный промпт"}]
while True:
    prompt = input("Введи текст: q - выйти. ")
    if prompt == "q":
        break # Выход из цикла и завершение программы
    messages.append({"role": "user","content": prompt}) # Добавление в список messages промпта
    response: ChatResponse = chat(model="gemma3:12b",messages=messages) # Здесь messages=messages - тот список. Идет передача промпта в ИИ
    answer = response.message.content # переменная с ответом от ИИ
    print(answer)
    messages.append({"role": "assistant","content": answer}) # передача ответа от ИИ в ИИ, в режиме ассистента
```



### Управление и взаимодействие с Ollama

При помощи модуля можно еще и по разному взаимодействовать с Ollama. Например загружать модели, смотреть список моделей и так далее.

```python
import ollama
ollama.chat(model='модель', messages=[{'role': 'user', 'content': 'промпт'}]) # Чат
ollama.generate(model='модель', prompt='промпт') # Простой запрос
ollama.list() # список загруженных моделей
ollama.ps() # Список загруженных в память моделей
ollama.show('модель') # Информация о модели
ollama.create(model='имяДляНовойМодели', from_='модель', system="системныйПромпт") # Создание модели на основе имеющейся
ollama.copy('модель', 'путь/имяСкопированнойМодели') # Копировать модель
ollama.delete('модель') # Удалить модель
ollama.pull('модель') # Загрузить модель из официального хаба
ollama.push('user/модель') # Отправить свою модель в официальный хаб
```



##  Обращение к API встроенными модулями

Так же есть возможность использовать Ollama при помощи языков программирования, например Python. Убедитесь, что у вас установлен интерпретатор языка. Если используете Docker, убедитесь, что интерпретатор установлен в контейнере или установите его.

Далее будет предоставлен простой пример. По сути все сводится к отправлению POST запроса и парсингу выходного json, к которому придется обратиться.

```python
import requests
import json
url="http://localhost:11434/api/generate"
headers={"content-type":"application/json"}
data={
"model":"модель",
"prompt":"запрос",
"stream":False
}
response=requests.post(url,headers=headers,data=json.dumps(data))
if response.status_code==200:
	response_text=response.text
	data=json.loads(response_text)
	actual_response=data["response"]
	print(actual_response)
else:
	print("Ошибка")
```

data содержит словарь. В примере выше было обращение к ключу response, чтобы получить содержимое ответа. Можно получить весь словарь, если принтовать его без ключей. Ниже будет описание полученных ключей.


| Ключ                     | Описание                                                                          |
| ------------------------ | --------------------------------------------------------------------------------- |
| **total_duration**       | Время потраченное на генерацию ответа                                             |
| **load_duration**        | Время затраченное на загрузку модели в наносекундах                               |
| **prompt_eval_count**    | Количество токенов в запросе                                                      |
| **prompt_eval_duration** | Время оценки запроса в наносекундах                                               |
| **eval_count**           | Количество токенов в ответе                                                       |
| **eval_duration**        | Время в наносекундах для генерации ответа                                         |
| **context**              | Контекст. Его можно вставить в кастомизацию для возможности продолжения разговора |
| **response**             | Ответ                                                                             |

### Перечень некоторых ключей для data-запроса


| Ключ           | Описание                                                                                                                                                                                                                                               |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **model**      | Модель                                                                                                                                                                                                                                                 |
| **prompt**     | Промпт                                                                                                                                                                                                                                                 |
| **message**    | более расширенный вариант создания промпта. В качестве значения принимает список с вложенным словарем. Далее будут перечислены ключи для этого вложенного словаря.<br>role - роль. Значением может быть user, assistant или tool.<br>content - промпт. |
| **stream**     | Если значение True, то ответ будет выдаваться потоком как в ChatGPT. Если False - ответ будет выдаваться сразу, что более предпочтительно при работе с API.                                                                                            |
| **keep_alive** | Продолжительность хранения модели в памяти. По умолчанию 5m. Если 0 - модель будет выгружена из памяти                                                                                                                                                 |
| **options**    | Дополнительные настройки, которые будут рассмотрены ниже. Принимаются некоторые параметры, что и при создании Modelfile.                                                                                                                               |






